{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iji', 'Ubube', 'Eme', 'Ihe', '—', 'Ị̀', 'Na', '-', 'eme', 'Nnyocha']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file= open(\"jw300.ig.txt\",encoding= \"utf8\")\n",
    "#file= open(\"test.ig.txt\",encoding= \"utf8\")\n",
    "\n",
    "filex= file.read()\n",
    "#filet= filex.lower()\n",
    "#filet= re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úù]\",'',filet)#Use regex to clean sentence\n",
    "file= filex.split()\n",
    "file[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ede59b314d6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstuff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mget_char\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filet' is not defined"
     ]
    }
   ],
   "source": [
    "x= \"Iji Ubube Eme Ihe — Ị̀ Na - eme Nnyocha Ndị A Iji Zere Mmerụ Ahụ ?\"\n",
    "\n",
    "\n",
    "#Code to get Characters in the file\n",
    "\n",
    "def get_char(my_file):\n",
    "    stuff=[]\n",
    "    for x in my_file:\n",
    "        stuff.append(x)\n",
    "    return set(stuff)\n",
    "\n",
    "get_char(filet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow= Counter(file)\n",
    "bow['chineke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_1_words(arr_of_sentences):\n",
    "    \n",
    "    a_list= []\n",
    "    list_of_single_word_sent= []\n",
    "    \n",
    "    for sent in arr_of_sentences:\n",
    "        #print(sent)\n",
    "        for word in sent.split():\n",
    "            #print (bow[word])\n",
    "            if bow[word]<=1:\n",
    "                #print(sent)\n",
    "                print(f'Word:{word:{12}}   |   Index:{arr_of_sentences.index(sent):{3}}  |   freq:{bow[word]:{3}}')\n",
    "                #print(arr_of_sentences.index(sent))\n",
    "                #a_list.append(arr_of_sentences.pop(arr_of_sentences.index(sent)))\n",
    "                a_list.append(arr_of_sentences.index(sent))\n",
    "                \n",
    "                \n",
    "    #print(arr_of_sentences)       \n",
    "    for x in set(a_list):\n",
    "        list_of_single_word_sent.append(arr_of_sentences[x])\n",
    "    \n",
    "    for i in list_of_single_word_sent:\n",
    "        arr_of_sentences.remove(i)\n",
    "\n",
    "    return arr_of_sentences\n",
    "    #return set(a_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search1= ['akụkọ ndị gaamasị gị',\n",
    " 'joseph achuzie dike biafra alala',\n",
    " 'dapchi gọọmenti emeribeghị boko haram  massob',\n",
    " 'tottenham naele anya iburu iko fa na mmeri rochadale',\n",
    " 'son heungmin fernando llorente na kyle walterpeters nke tottenham bụ ndị mmechiri ọnụ rochadale nasọmpị ụnyaahụ',\n",
    " 'son heungmin onye tottenham na ndị otu ya nanwe anụrị mgbe o nyere goolu nasọmpị iko fa',\n",
    " 'tottenham emerila nasọmpị iko fa ugboro asatọ',\n",
    " 'negwuregwu ndị otu tottenham gbara ndị rochadale ọkpụ goolu isii asatara otu nasọmpi fa',\n",
    " 'son heungmin nyere goolu nke mbụ na nke ise fernando llorente nyere goolu atọ nime nkeji iri na abụọ nokara nke abụọ ebe kyle walterpeters nyere goolu nke isi',\n",
    " 'son heungmin kwụsị bọọlụ otu rochadale']\n",
    "\n",
    "''' \n",
    "for w in search:\n",
    "    search.pop(search.index(w))\n",
    "       \n",
    "#search.pop(3)\n",
    "search.pop(4)\n",
    "\n",
    "len(search)\n",
    "search\n",
    "'''\n",
    "#hmm= {0, 1, 2, 3}\n",
    "#hmm2=[]\n",
    "\n",
    "#for i in hmm:\n",
    "    #hmm2.append(search1[i])\n",
    "    #del search1[i] #using del means as the loop is updating the indexes are changing and it is del that way\n",
    "    \n",
    "#hmm2\n",
    "\n",
    "#for i in hmm2:\n",
    "    #search1.remove(i)\n",
    "    \n",
    "#search1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= remove_1_words(search1)\n",
    "#x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(search1)\n",
    "#len(ans)\n",
    "x= 'akụkọ ndị gaamasị gị'\n",
    "x.split()\n",
    "\n",
    "y=[0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]\n",
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning My Dataset\n",
    "\n",
    "The codes below perform the ffg funtion\n",
    "\n",
    "1. cleaner(): Removes symbols and use lower()\n",
    "2. remove_rare_words(): Remove words with less than one occurence\n",
    "3. removes spaces in the text using .strip().replace(\"  \",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A code to remove puntuations and symbols except ('), and to bring data to a lower case\n",
    "\n",
    "def cleaner(a_sentence):\n",
    "    \n",
    "    sentence= a_sentence.split('\\n') #split data into sentences\n",
    "    new_array=[]\n",
    "    \n",
    "    for sent in sentence:\n",
    "        #print(sent)\n",
    "        if re.search(r'\\d', sent) or len(sent) < 2: #Ignore sentences with numbers, and len <2\n",
    "            #print(sent)\n",
    "            pass\n",
    "        else:\n",
    "            clean_punct= re.sub(r\"[^\\w\\s]\",'',sent) #remove punctuations and symbols except (’)\n",
    "            #clean_punct= re.sub(r\"[^\\w\\s’]\",'',sent) #remove punctuations and symbols except (’)\n",
    "            #clean_punct= re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úùṅṇÄÁÀÉÈỌỌ̀ÒÓỊ̀ỊÌỤÚṄŃḾ’']\",'',sent)\n",
    "            new_array.append(clean_punct.lower())\n",
    "            #sentence.remove(sent)\n",
    "                  \n",
    "    new_array=new_array[:]\n",
    "    #return new_array\n",
    "    return \"\\n\".join(new_array)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neat version of the code without the tests\n",
    "\n",
    "# CHATGPT code to remove one_word occurence\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def remove_rare_words(corpus):\n",
    "    # Split the corpus into sentences\n",
    "    #sentences = re.split(r'[.!?]', corpus)\n",
    "    sentences = corpus.split('\\n')\n",
    "    \n",
    "    # Create a Counter object to count the occurrences of each word\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    #print (word_counts)\n",
    "    # Remove sentences that contain words that occur only once\n",
    "    #new_corpus = \"\"\n",
    "    new_corpus=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        include_sentence = True\n",
    "        for word in sentence.split():\n",
    "            if word_counts[word] == 1:\n",
    "                include_sentence = False\n",
    "                break\n",
    "        if include_sentence:\n",
    "            #new_corpus += sentence + \" \" \n",
    "            new_corpus.append(sentence)\n",
    "    \n",
    "        \n",
    "    #return new_corpus.strip()\n",
    "    return \"\\n\".join(new_corpus),word_counts # returning the \"x\".strip().replace(\"  \",\" \") to remove extra spaces does not work when used within the function\n",
    "    \n",
    "\n",
    "#corpus = \"This is a sentence. This is another sentence. This is a unique sentence.\"\n",
    "#print(remove_rare_words(corpus))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'senta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fb8f0dcaf3b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#x= remove_rare_words(senta)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msenta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(senta)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print (x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'senta' is not defined"
     ]
    }
   ],
   "source": [
    "#x= remove_rare_words(senta)\n",
    "x= cleaner(senta)\n",
    "\n",
    "#print(senta)\n",
    "#print (x)\n",
    "\n",
    "#print(cleaner(x))\n",
    "print (remove_rare_words(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-663779c458ad>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-663779c458ad>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    y=\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = {'na': 135, 'ọ': 75, 'ya': 58, 'ubube': 41, 'ahụ': 41, 'ka': 39, 'ị': 39, 'ma': 38, 'ihe': 37, 'a': 34, 'ndị': 32, 'dị': 32, 'nke': 29, 'mgbe': 29, 'ga': 28, 'bụrụ': 22, 'gị': 19, 'bụ': 18, 'ha': 17, 'ebe': 16, 'eme': 15, 'ọrụ': 15, 'e': 15, 'mmadụ': 15, 'aka': 14, 'mee': 13, 'iji': 12, 'elu': 12, 'pụrụ': 12, 'ụkwụ': 12, 'ike': 12, 'i': 12, 'ji': 11, 'o': 10, 'n’elu': 10, 'n’ihe': 10, 'otu': 10, 'anya': 9, 'arụ': 8, 'otú': 8, 'ò': 8, 'azọdo': 8, 'bụla': 8, 'nile': 8, 'ala': 8, 'adịla': 8, 'site': 7, 'abụọ': 7, 'n’ihi': 6, 'eji': 6, 'n’ebe': 6, 'apịachi': 6, 'aga': 6, 'buru': 6, 'jiri': 6, 'jide': 6, 'mmerụ': 5, 'onye': 5, 'ụlọ': 5, 'oge': 5, 'rụọ': 5, 'ọma': 5, 'ókè': 5, 'ogologo': 5, 'n’ime': 5, 'kegide': 5, 'ọzọ': 5, 'paul': 4, 'eletrik': 4, 'nwere': 4, 'daa': 4, 'ukwuu': 4, 'banyere': 4, 'ime': 4, 'ghaghị': 4, 'kwesịrị': 4, 'n’ubube': 4, 'hụ': 4, 'nanị': 4, 'gafee': 4, 'atọ': 4, 'mbụ': 4, 'n’ala': 4, 'ngwá': 4, 'zere': 3, 'n’aka': 3, 'mkpa': 3, 'bọlb': 3, 'n’ihu': 3, 'ịkpata': 3, 'ụzọ': 3, 'si': 3, 'tupu': 3, 'mma': 3, 'onwe': 3, 'ize': 3, 'ndụ': 3, 'arịba': 3, 'ekwesị': 3, 'agbabewe': 3, 'osisi': 3, 'kọrọ': 3, 'ghara': 3, 'hà': 3, 'ọnụ': 3, 'eriri': 3, 'nakwa': 3, 'n’ụzọ': 3, 'ụgbọala': 3, 'n’azụ': 3, 'mfe': 3, 'ịhụ': 3, 'n’ubu': 3, 'ọchị': 3, 'adịghị': 3, 'mere': 3, 'maka': 3, 'larịị': 3, 'rụnyere': 3, 'arịgo': 3, 'nọ': 3, 'ekwe': 3, 'ọbụna': 3, 'ọhụrụ': 3, 'ịtọrọ': 3, 'ì': 2, 'nnyocha': 2, 'ọkụ': 2, 'dịkwa': 2, 'ihicha': 2, 'windo': 2, 'n’ụlọ': 2, 'n’èzí': 2, 'gịnị': 2, 'ezi': 2, 'isi': 2, 'aro': 2, 'iri': 2, 'nyeere': 2, 'n’emerụghị': 2, 'ịbụ': 2, 'ịgbatị': 2, 'gabiga': 2, 'dịkarịrị': 2, 'n’uko': 2, 'debe': 2, 'aṅagharị': 2, 'egosi': 2, 'adị': 2, 'n’okpuru': 2, 'jikọtara': 2, 'inwe': 2, 'mkpọre': 2, 'chọrọ': 2, 'oghere': 2, 'tinyere': 2, 'egbochi': 2, 'dịkarịa': 2, 'ná': 2, 'dịrị': 2, 'agbatị': 2, 'bu': 2, 'were': 2, 'ihu': 2, 'n’uche': 2, 'n’ogologo': 2, 'guzowere': 2, 'arọ': 2, 'kpachara': 2, 'amị': 2, 'eguzowe': 2, 'arụnye': 2, 'mita': 2, 'rịgoro': 2, 'nrịgo': 2, 'ikpeazụ': 2, 'bú': 2, 'gwa': 2, 'siri': 2, 'yiri': 2, 'emi': 2, 'anọ': 2, 'n’ike': 2, 'mịchapụ': 2, 'arị': 2, 'ọsọ': 2, 'ọnọdụ': 2, 'nwee': 2, 'ngwa': 2, 'mara': 2, 'ruo': 2, 'n’oge': 2, 'zuru': 2, 'ụwa': 2, 'gara': 2, 'omempụ': 2, 'ntọ': 2, 'agagharị': 2, 'anụ': 2, 'nta': 1, 'akụkọ': 1, 'teta': 1, 'ireland': 1, 'ịgbanwe': 1, 'nwunye': 1, 'ekwuwo': 1, 'ọtụtụ': 1, 'ugboro': 1, 'nọgidere': 1, 'eyigharị': 1, 'uche': 1, 'ịtụ': 1, 'egwu': 1, 'maara': 1, 'ọbụnakwa': 1, 'ọnwụ': 1, 'echeghị': 1, 'echiche': 1, 'ziri': 1, 'tụlee': 1, 'mkpụmkpụ': 1, 'ịgbabiwe': 1, 'è': 1, 'debere': 1, 'agbasa': 1, 'dere': 1, 'mmiri': 1, 'akpachikọlatakwa': 1, 'mmirị': 1, 'isikwa': 1, 'gbawara': 1, 'agbawa': 1, 'ereela': 1, 'enwekarị': 1, 'ígwè': 1, 'sie': 1, 'sikwara': 1, 'ntú': 1, 'kụdo': 1, 'agbajiela': 1, 'taa': 1, 'nchara': 1, 'akaala': 1, 'rụzie': 1, 'nrụzi': 1, 'gbanwee': 1, 'mgbanwe': 1, 'ozugbo': 1, 'etinyekarị': 1, 'ịmịchapụ': 1, 'wepụrụ': 1, 'unyi': 1, 'ijupụta': 1, 'n’ụkwụ': 1, 'ichipụ': 1, 'echipụ': 1, 'emebibeghị': 1, 'emebi': 1, 'ekedo': 1, 'adọkpụ': 1, 'ịpụta': 1, 'ákwà': 1, 'ịdọ': 1, 'ntị': 1, 'eso': 1, 'akasị': 1, 'eburịrị': 1, 'chọọ': 1, 'jidesie': 1, 'kwụzie': 1, 'ịkụ': 1, 'bulie': 1, 'ogo': 1, 'azụ': 1, 'kụrụ': 1, 'nkiri': 1, 'yie': 1, 'bịa': 1, 'n’ezie': 1, 'karịsịa': 1, 'ọtọ': 1, 'weliekwa': 1, 'kwụrụ': 1, 'lezie': 1, 'waya': 1, 'saịn': 1, 'bọọdụ': 1, 'dịghị': 1, 'ugegbe': 1, 'plastik': 1, 'kwere': 1, 'kegidesie': 1, 'gbabewere': 1, 'nnọọ': 1, 'kegidere': 1, 'adịkwa': 1, 'arịdata': 1, 'ketusịrị': 1, 'adịkarịghị': 1, 'nyere': 1, 'irè': 1, 'akarịghị': 1, 'ise': 1, 'kpọdara': 1, 'akpọda': 1, 'hinye': 1, 'guzoro': 1, 'ibé': 1, 'gbadosiri': 1, 'gbasaa': 1, 'pịchie': 1, 'ịpịachi': 1, 'nchebe': 1, 'okpuru': 1, 'akpụkpọ': 1, 'wepụ': 1, 'apịtị': 1, 'ịrịgo': 1, 'tinye': 1, 'akpa': 1, 'belt': 1, 'n’úkwù': 1, 'gbalịa': 1, 'ịchọta': 1, 'esi': 1, 'ebugo': 1, 'obubu': 1, 'ejirịrị': 1, 'ejighị': 1, 'ejide': 1, 'n’ịdị': 1, 'achị': 1, 'n’akụkụ': 1, 'nwayọọ': 1, 'arikwala': 1, 'atụ': 1, 'akpọpu': 1, 'ịtọ': 1, 'wee': 1, 'nọhie': 1, 'agbachi': 1, 'ịda': 1, 'agafe': 1, 'bụrụkwa': 1, 'gebichie': 1, 'iburu': 1, 'gbagọrọ': 1, 'agbagọ': 1, 'enweta': 1, 'mberede': 1, 'kwuo': 1, 'okwu': 1, 'leekwa': 1, 'nọchiri': 1, 'rịgoo': 1, 'obere': 1, 'screwdriver': 1, 'ịpụ': 1, 'nweghị': 1, 'ikegide': 1, 'legide': 1, 'tọgbọ': 1, 'agaghị': 1, 'emerụ': 1, 'lọghachiri': 1, 'ahapụla': 1, 'n’enweghị': 1, 'legidere': 1, 'nkwụzi': 1, 'n’usoro': 1, 'enye': 1, 'ịrị': 1, 'arịla': 1, 'arịa': 1, 'ọrịa': 1, 'agbọ': 1, 'enu': 1, 'ájù': 1, 'ebu': 1, 'nọrọ': 1, 'lee': 1, 'ezere': 1, 'karịa': 1, 'rịa': 1, 'oké': 1, 'ifufe': 1, 'eguzo': 1, 'edebe': 1, 'afọdụ': 1, 'amatịbiga': 1, 'gbatịbiga': 1, 'anọhie': 1, 'kama': 1, 'merụọ': 1, 'bugharịa': 1, 'ewe': 1, 'karịrị': 1, 'ele': 1, 'n’agbanyeghị': 1, 'akpacharu': 1, 'enwe': 1, 'ikekwe': 1, 'inyere': 1, 'ibelata': 1, 'n’ịgbaso': 1, 'n’ọkụ': 1, 'gịnikwa': 1, 'eleghị': 1, 'achụmnta': 1, 'ego': 1, 'afọ': 1, 'amụbawo': 1, 'gburugburu': 1, 'mpụ': 1, 'ewuru': 1, 'n’ụwa': 1, 'russia': 1, 'philippines': 1, 'njikere': 1, 'ịnwụde': 1, 'n’otu': 1, 'tọọrọ': 1, 'nwa': 1, 'ụbọchị': 1, 'ụmụ': 1, 'anụmanụ': 1, 'eyighị': 1, 'nweere': 1, 'sịrị': 1, 'òtù': 1, 'mexico': 1, 'agba': 1, 'batara': 1, 'n’òtù': 1, 'ume': 1, 'enyi': 1, 'amụgharị': 1, 'nweta': 1, 'ahụmahụ': 1, 'ezu': 1, 'ebido': 1, 'ezigbo': 1}\n",
    "\n",
    "y= Counter({'na': 135, 'ọ': 75, 'ya': 58, 'ubube': 41, 'ahụ': 41, 'ka': 39, 'ị': 39, 'ma': 38, 'ihe': 37, 'a': 34, 'ndị': 32, 'dị': 32, 'nke': 29, 'mgbe': 29, 'ga': 28, 'bụrụ': 22, 'gị': 19, 'bụ': 18, 'ha': 17, 'ebe': 16, 'eme': 15, 'ọrụ': 15, 'e': 15, 'mmadụ': 15})\n",
    "y=\n",
    "x['agbabewe']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaner(remove_rare_words(x))\n",
    "\n",
    "ọ bụ n’ihi na ọ ga  eji ubube rụọ ha \n",
    "ndị a bụ aro iri nyeere ya aka ime ihe n’emerụghị ahụ \n",
    "zere iji ubube a na  apịachi apịachi arịba n’uko ụlọ gị \n",
    "ubube abụọ e jikọtara ọnụ pụrụ inwe ihe mkpọre na eriri \n",
    "ihe mkpọre ndị ahụ hà na  arụ ọrụ nke ọma \n",
    "kpachara anya maka ebe ị na  agbabewe ebe elu ubube \n",
    "ọ bụrụ na i ji ngwá ọrụ ji ike eletrik eme ihe na  arụ ọrụ  ya adịla mgbe ị ga  eji aka abụọ jide ya ka ị na  arụ ọrụ \n",
    "\n",
    "remove_rare_words(cleaner(x))\n",
    "iji ubube eme ihe  ì na  eme nnyocha ndị a iji zere mmerụ ahụ \n",
    "n’ihi gịnị \n",
    "ọ bụ n’ihi na ọ ga  eji ubube rụọ ha \n",
    "ndị a bụ aro iri nyeere ya aka ime ihe n’emerụghị ahụ \n",
    "ubube ahụ ò dị mma \n",
    "zere iji ubube a na  apịachi apịachi arịba n’uko ụlọ gị \n",
    "debe ubube kwesịrị ekwesị a ga na  eji arịba n’uko ụlọ gị  ma ọ bụ na  eji ubube a na  agbabewe agbabewe arịba \n",
    "ubube abụọ e jikọtara ọnụ pụrụ inwe ihe mkpọre na eriri \n",
    "ihe mkpọre ndị ahụ hà na  arụ ọrụ nke ọma \n",
    "kpachara anya maka ebe ị na  agbabewe ebe elu ubube \n",
    "ọ bụrụ na i ji ngwá ọrụ ji ike eletrik eme ihe na  arụ ọrụ  ya adịla mgbe ị ga  eji aka abụọ jide ya ka ị na  arụ ọrụ \n",
    "ha nyeere paul aka \n",
    "n’oge gara aga  ndị ntọ na  e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open jw300.ig.text file for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open(\"jw300.ig.txt\", encoding=\"utf8\")\n",
    "#file= open(\"test.ig.txt\", encoding=\"utf8\")\n",
    "#file= open(\"text_file_modified.txt\", encoding=\"utf8\")\n",
    "\n",
    "sent= file.read()\n",
    "#sent= sent.split('\\n')\n",
    "#sent= re.sub(r\"[^\\w\\s'][’]\",'',sent)\n",
    "senta=sent[:10000]\n",
    "#senta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ \\nsite naka onye nta akụkọ teta \\nna irela'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#half_clean_file2, diction= cleaner(remove_rare_words(sent))\n",
    "\n",
    "half_clean_file, diction= remove_rare_words(cleaner(sent)) \n",
    "\n",
    "Cleaned_file= half_clean_file.strip().replace(\"  \",\" \")\n",
    "Cleaned_file[:100]\n",
    "\n",
    "#Cleaned_file_araba= clean_file_for_araba_data.strip().replace(\"  \",\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction[\"myrtle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open('my_corpus.txt', 'w+', encoding='utf8')#half_clean_file\n",
    "#file= open('my_corpus2.txt', 'w+', encoding='utf8')#half_clean_file2\n",
    "#file= open('test.ig.with.errors.txt', 'w+', encoding='utf8')#half_clean_file\n",
    "#file= open('test.ig.txt', 'w+', encoding='utf8')#half_clean_file\n",
    "\n",
    "\n",
    "file.write(Cleaned_file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for Araba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Igbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the section for the dataset, I want to give to Araba for data validation\n",
    "\n",
    "clean_file_for_araba_data= remove_rare_words(sent)\n",
    "\n",
    "Cleaned_file_araba= clean_file_for_araba_data.strip().replace(\"  \",\" \")\n",
    "\n",
    "file= open('my_corpus_araba.txt', 'w+', encoding='utf8')\n",
    "file.write(Cleaned_file_araba)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file= open(\"jw300.en.txt\", encoding=\"utf8\")\n",
    "file= open(\"test.ig.txt\", encoding=\"utf8\")\n",
    "\n",
    "sent= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the section for the dataset, I want to give Araba for data validation English\n",
    "clean_file_for_araba_data= remove_rare_words(sent)\n",
    "\n",
    "Cleaned_file_araba= clean_file_for_araba_data.strip().replace(\"  \",\" \")\n",
    "\n",
    "file= open('my_corpus_araba_eng.txt', 'w+', encoding='utf8')\n",
    "file.write(Cleaned_file_araba)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ \\nsite n’aka onye nta akụkọ teta'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tesing Cleaner\n",
    "x='Iji Ubube Eme Ihe — Ị̀ Na - eme Nnyocha Ndị A Iji Zere Mmerụ Ahụ ?\\nSite n’aka onye nta akụkọ Teta !'\n",
    "y= cleaner(x)\n",
    "\n",
    "z= '  iji ubube eme ihe  ì na  eme nnyocha ndị a iji zere mmerụ ahụ \\nsite n’aka onye nta akụkọ teta '\n",
    "q= z.strip().replace(\"  \",\" \")\n",
    "\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to keep the (') in the data\n",
    "#x= \"Iguzo mwuli akọn////auche nke ndị mmadụ site n'ịchọ mgbanwe abụghị ịda iwu\"\n",
    "x= \"Ọ bụ n’ihi na ọ ga - eji ubube rụọ ha\"\n",
    "#x= \"Iji Ubube Eme Ihe — Ị̀ Na - eme Nnyocha Ndị A Iji Zere Mmerụ Ahụ ?\"\n",
    "#x= re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úùṅṇÄÁÀÉÈỌỌ̀ÒÓỊ̀ỊÌỤÚṄŃḾ’]\",'',x)\n",
    "x= re.sub(r\"[^\\w\\s'][’]\",'',x) # this is the better code \n",
    "x\n",
    "\n",
    "#the method works for the dummy datasets but not for the jwIg300.txt dataset, why is because I have not completed the regex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b0b91a441716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Define the model architecture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=hidden_dim, return_sequences=True, recurrent_dropout=0.5))\n",
    "model.add(LSTM(units=hidden_dim, recurrent_dropout=0.5))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the spell correction dataset\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "# Use the trained model for context-sensitive spell correction\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "In this code, we first import the necessary layers and models from Keras. We then define the architecture of the BiLSTM model, with an embedding layer, two LSTM layers, and a dense output layer. We set the recurrent_dropout to 0.5 to prevent overfitting.\n",
    "\n",
    "We then compile the model, specifying the optimizer, loss function, and evaluation metric. After that, we train the model on the spell correction dataset using the fit method. Finally, we use the trained model to make context-sensitive spell correction predictions on the test dataset.\n",
    "\n",
    "This is just a sample code, the actual implementation might vary depending on the specific dataset and problem you are working on. You need to have a trained dataset to use the above code.\n",
    "\n",
    "You might also want to fine-tune the parameters like vocab_size, embedding_dim, max_length, hidden_dim, batch_size, num_epochs based on your dataset.\n",
    "\n",
    "\n",
    "#### more\n",
    "Yes, this is a code for a Bi-LSTM model for spell correction using Keras library. It creates a sequential model, adds an embedding layer to handle the input sequences, followed by two LSTM layers with recurrent dropout for regularization, and a dense layer with 'softmax' activation for predictions. The model is then compiled using 'adam' optimizer and 'categorical_crossentropy' loss function. Finally, it is trained on the spell correction dataset using the specified batch size and number of epochs.\n",
    "\n",
    "\n",
    "## Y-train\n",
    "To get your \"y_train\" for training the spell correction model, you need to preprocess your text corpus into the correct format. This involves several steps, such as tokenizing the text into words, creating a vocabulary of all the unique words, converting the words into numerical representations, and padding the sequences to a uniform length.\n",
    "\n",
    "Here is an example of how to get \"y_train\" using the text corpus:\n",
    "\n",
    "1. Tokenize the text into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-202fab2ce1c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Tokenize the text into words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_corpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text into words\n",
    "text_tokens = [word_tokenize(sent) for sent in text_corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a vocabulary of all the unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a6d9a2bd41cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Get a list of all the words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Create a vocabulary of all the unique words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get a list of all the words\n",
    "all_words = [word for tokens in text_tokens for word in tokens]\n",
    "\n",
    "# Create a vocabulary of all the unique words\n",
    "word_counter = Counter(all_words)\n",
    "\n",
    "# Get the most common words\n",
    "vocab = [word for word, count in word_counter.most_common(vocab_size - 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert the words into numerical representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-445bdcbda0c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Convert the words into numerical representations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<OOV>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Convert the words into numerical representations\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(text_tokens)\n",
    "sequences = tokenizer.texts_to_sequences(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pad the sequences to a uniform length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2b8246a1f946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Pad the sequences to a uniform length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpadded_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sequences to a uniform length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, you can set y_train to be equal to padded_sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-066c994b32f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadded_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'padded_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "y_train = padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
