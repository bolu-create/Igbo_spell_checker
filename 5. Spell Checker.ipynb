{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to store a Trie node\n",
    "class Trie:\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.isLeaf = False\n",
    "        self.children = {}\n",
    " \n",
    " \n",
    "    # Iterative function to insert a string into a Trie\n",
    "    def insert(self, key):\n",
    "        #print('Inserting…', key)\n",
    " \n",
    "        # start from the root node\n",
    "        curr = self\n",
    " \n",
    "        # do for each character of the key\n",
    "        for c in key:\n",
    "            # go to the next node and create one if the path doesn't exist\n",
    "            curr = curr.children.setdefault(c, Trie())\n",
    " \n",
    "        # mark the current node as a leaf\n",
    "        curr.isLeaf = True\n",
    " \n",
    " \n",
    "    # Iterative function to search a key in a Trie. It returns True\n",
    "    # if the key is found in the Trie; otherwise, it returns False\n",
    "    def search(self, key):\n",
    " \n",
    "        #print('Searching', key, end=': ')\n",
    "        curr = self\n",
    " \n",
    "        # do for each character of the key\n",
    "        for c in key:\n",
    "            # go to the next node\n",
    "            curr = curr.children.get(c)\n",
    "            # if the string is invalid (reached end of a path in the Trie)\n",
    "            if curr is None:\n",
    "                return False\n",
    " \n",
    "        # return true if the current node is a leaf node, and we have reached\n",
    "        # the end of the string\n",
    "        return curr.isLeaf\n",
    "    \n",
    "    \n",
    "    \n",
    "# Creating an Instance of the class trie\n",
    "t = Trie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28538\n",
      "['mgbe', 'jezibel', 'nwụsịrị', 'eze', 'jihu', 'egbughị', 'oge', 'ọ', 'bụla', 'nigbusịsị']\n"
     ]
    }
   ],
   "source": [
    "#Open the training corpus\n",
    "\n",
    "file= open(\"train.ig.txt\", encoding=\"utf8\")\n",
    "file_read= file.read()\n",
    "file_to_count= file_read.split()\n",
    "my_file= list(set(file_to_count))\n",
    "file_count= len(my_file)\n",
    "print(file_count)\n",
    "print(file_to_count[:10])\n",
    "file.close()\n",
    "\n",
    "my_list_of_words= my_file\n",
    "#my_list_of_words=[\"ọ\", \"dị\", \"paul\", \"mkpa\", \"ịgbanwe\", \"bọlb\", \"ọkụ\", \"eletrik\", \"dị\", \"nihu\", \"ụlọ\", \"ya\"]\n",
    "\n",
    "#Ọ bụ n’ihi na ọ ga eji ubube rụọ ha\n",
    "#ọ dị paul mkpa ịgbanwe bọlb ọkụ eletrik dị nihu ụlọ ya\n",
    "#Iji Ubube Eme Ihe — Ị̀ Na - eme Nnyocha Ndị A Iji Zere Mmerụ Ahụ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in my_list_of_words:\n",
    "    t.insert(word)\n",
    "    \n",
    "t.search(\"mmadụ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the models\n",
    "\n",
    "#Using Model files to call ngram. (This causes my computer to hang)\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "def _my_normalization():\n",
    "    return defaultdict(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(\"Tri_Left.pkl\", \"rb\") as pkl_handle:\n",
    "\tmodel_Tri_Left = pickle.load(pkl_handle)\n",
    "    \n",
    "with open(\"Tri_Right.pkl\", \"rb\") as pkl_handle:\n",
    "\tmodel_Tri_Right = pickle.load(pkl_handle)\n",
    "    \n",
    "with open(\"Tri_Center.pkl\", \"rb\") as pkl_handle:\n",
    "\tmodel_Tri_Center = pickle.load(pkl_handle)\n",
    "    \n",
    "with open(\"Bi_Left.pkl\", \"rb\") as pkl_handle:\n",
    "\tmodel_Bi_Left = pickle.load(pkl_handle)\n",
    "    \n",
    "with open(\"Bi_Right.pkl\", \"rb\") as pkl_handle:\n",
    "\tmodel_Bi_Right = pickle.load(pkl_handle)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Model Candidate Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGRAM RESULTS\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "\n",
    "def func_of_ifs(dici_list):\n",
    "    index_mod=[]\n",
    "    counter= collections.Counter()\n",
    "    \n",
    "    for mod in dici_list:#then get the indexes of words that are correct\n",
    "        if mod:\n",
    "            #print(mod)\n",
    "            index_mod.append(dici_list.index(mod))\n",
    "    for dict_el in dici_list:\n",
    "        if dict_el:\n",
    "            counter.update(dict_el)\n",
    "            \n",
    "    return counter,index_mod\n",
    "\n",
    "\n",
    "def ngram_multi_sugg(sent,arr_index):\n",
    "    \n",
    "    #sent_clean= re.sub(r'[^A-Za-z àáéèọíìịụụ́ụ̀úù]','',sent) #remove punctuation and numbers. I have to add Igbo characters to this??!\n",
    "    sent_clean=re.sub(r\"[^\\w\\s’]\",'',sent)#Use regex to clean sentence\n",
    "    arr_sent= sent_clean.split()# make sent an array\n",
    "    index_mod=''\n",
    "    #counter= collections.Counter()\n",
    "    counter=''\n",
    "    ngram_result= {}\n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    for word in arr_sent:\n",
    "        # Check all the possible ngram variations of the word\n",
    "       \n",
    "        \n",
    "        if word == arr_sent[arr_index]:\n",
    "            \n",
    "            #if len(arr_sent) < 2:\n",
    "                #pass\n",
    "            \n",
    "            if len(arr_sent) == 2:\n",
    "                \n",
    "                if arr_index== 0: #first word\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    first_word= bi_left\n",
    "                    ngram_result= first_word\n",
    "                \n",
    "                if arr_index==1: #second word\n",
    "                    bi_right= dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    second_word= bi_right\n",
    "                    ngram_result= second_word\n",
    "                   \n",
    "                \n",
    "              \n",
    "            if len(arr_sent) == 3:\n",
    "                \n",
    "                if arr_index== 0: #first word\n",
    "                    tri_left= dict(model_Tri_Left[arr_sent[arr_index+1],arr_sent[arr_index+2]])\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    first_word= [tri_left,bi_left]\n",
    "                    counter,index_mod= func_of_ifs(first_word)\n",
    "            \n",
    "                elif arr_index==1: #second word\n",
    "                    bi_right= dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    second_word= [bi_right,bi_left]\n",
    "                    counter,index_mod= func_of_ifs(second_word)\n",
    "                    \n",
    "                elif arr_index == 2: #third word\n",
    "                    bi_right = dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    tri_right = dict(model_Tri_Right[arr_sent[arr_index-2],arr_sent[arr_index-1]])\n",
    "                    third_word = [bi_right, tri_right]\n",
    "                    counter,index_mod= func_of_ifs(third_word)\n",
    "                                 \n",
    "                counter= dict(counter)\n",
    "\n",
    "                for el in counter:\n",
    "                    ngram_result[el]=counter[el]/len(index_mod)\n",
    "                  \n",
    "                \n",
    "                                  \n",
    "            if len(arr_sent)>3:         \n",
    "            \n",
    "                if arr_index == 0: #first word\n",
    "                    tri_left= dict(model_Tri_Left[arr_sent[arr_index+1],arr_sent[arr_index+2]])\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    first_word= [tri_left,bi_left]\n",
    "                    counter,index_mod= func_of_ifs(first_word)\n",
    "\n",
    "\n",
    "                elif arr_index == 1: #secondword\n",
    "                    tri_center= dict(model_Tri_Center[arr_sent[arr_index-1],arr_sent[arr_index+1]])\n",
    "                    bi_right= dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    tri_left= dict(model_Tri_Left[arr_sent[arr_index+1],arr_sent[arr_index+2]])\n",
    "                    second_word= [tri_center,bi_right,bi_left,tri_left]\n",
    "                    counter,index_mod= func_of_ifs(second_word)\n",
    "\n",
    "\n",
    "                elif arr_index == len(arr_sent)-2: #second to the last\n",
    "                    tri_center= dict(model_Tri_Center[arr_sent[arr_index-1],arr_sent[arr_index+1]])\n",
    "                    bi_right= dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    tri_right= dict(model_Tri_Right[arr_sent[arr_index-2],arr_sent[arr_index-1]])\n",
    "                    second_to_last_word= [tri_center,bi_left,bi_right,tri_right]\n",
    "                    counter,index_mod= func_of_ifs(second_to_last_word)\n",
    "\n",
    "                elif arr_index == len(arr_sent)-1: #last word\n",
    "                    tri_right= dict(model_Tri_Right[arr_sent[arr_index-2],arr_sent[arr_index-1]])\n",
    "                    bi_right= dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    last_word= [tri_right,bi_right]\n",
    "                    counter,index_mod= func_of_ifs(last_word)\n",
    "\n",
    "                else:\n",
    "                    tri_right= dict(model_Tri_Right[arr_sent[arr_index-2],arr_sent[arr_index-1]])\n",
    "                    tri_left= dict(model_Tri_Left[arr_sent[arr_index+1],arr_sent[arr_index+2]])\n",
    "                    tri_center= dict(model_Tri_Center[arr_sent[arr_index-1],arr_sent[arr_index+1]])\n",
    "                    bi_right= dict(model_Bi_Right[arr_sent[arr_index-1]])\n",
    "                    bi_left= dict(model_Bi_Left[arr_sent[arr_index+1]])\n",
    "                    all_others= [tri_right,tri_left,tri_center,bi_right,bi_left] #put them in a list\n",
    "                    counter,index_mod= func_of_ifs(all_others)\n",
    "                        \n",
    "                    \n",
    "                counter= dict(counter)\n",
    "\n",
    "                for el in counter:\n",
    "                    ngram_result[el]=counter[el]/len(index_mod)                \n",
    "   \n",
    "    return  ngram_result\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # PROBLEMS: Any of the models that runs out of words cannot be used as they will generate index errors. Hence, \n",
    "    # all the models cannot be used with all the words. The models used for the cumulative calculation should take into\n",
    "    # the position of the words they are addressing just like in ngram_suggestion() above.\n",
    "    \n",
    "    #The regex is bad at identifying \n",
    "    \n",
    "    # LINK: https://tutorial.eyehunts.com/python/python-dictionary-sum-values-with-the-same-key-example-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Suggestions with Leveinsthein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "def candidate_suggestions (word,error_word):\n",
    "    \n",
    "    one_levensthein_words=[]\n",
    "    \n",
    "    words=list(word)\n",
    "    top_words={}\n",
    "    \n",
    "    for el in words:\n",
    "        if not el:\n",
    "            words.pop(words.index(el))\n",
    "\n",
    "    for key in words:\n",
    "        #print(key) \n",
    "        #print(enchant.utils.levenshtein(key, error_word))    \n",
    "        if enchant.utils.levenshtein(key, error_word)<=1 or enchant.utils.levenshtein(key, error_word)<=2:\n",
    "            one_levensthein_words.append(key)\n",
    "            \n",
    "            \n",
    "    for el in list(set(one_levensthein_words)):\n",
    "        if el in words:\n",
    "            top_words[el]= word[el]\n",
    "           \n",
    "    top_words= sorted(top_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words= top_words[:5]\n",
    "    top_words_and_probabilities = dict(top_words) # This gives the top words as well as the necessary probabilities\n",
    "    top_words= list(dict(top_words)) \n",
    "    \n",
    "    return top_words, top_words_and_probabilities\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Suggestions with Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def norvig_candidates(my_ngram,err_word):\n",
    "def candidate_suggestions(my_ngram,err_word):\n",
    "\n",
    "    \n",
    "    import enchant\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    \n",
    "    words= list(my_ngram)\n",
    "    top_words={}\n",
    "    \n",
    "    def ngram_result(your_ngram):\n",
    "        #open wordlist\n",
    "        words_string=[]\n",
    "        \n",
    "        for key in your_ngram:\n",
    "            words_string.append(key)\n",
    "        return words_string\n",
    "        #def words(text): return re.findall(r'\\w+', text.lower())\n",
    "        #def words(text): return re.findall(r'\\w+', lower_my_list(words_from_ngram_list))\n",
    "\n",
    "    #WORDS = Counter(words(open('igbo_clean_main.txt', encoding= 'utf8').read()))\n",
    "    \n",
    "    #words_from_ngram = file_to_count\n",
    "    words_from_ngram_list = ngram_result(my_ngram)\n",
    "    \n",
    "    WORDS = Counter(words_from_ngram_list)\n",
    "    \n",
    "   \n",
    "\n",
    "    def P(word, N=sum(WORDS.values())): \n",
    "        \"Probability of `word`.\"\n",
    "        return WORDS[word] / N\n",
    "\n",
    "    def correction(word): \n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(candidates(word), key=P)\n",
    "\n",
    "    def candidates(word): \n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        \"\"\"or known(edits3(word))\"\"\"#Add to the return statement below to run for 3 edit depth\n",
    "        return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "    def known(words): \n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in WORDS)\n",
    "\n",
    "    def edits1(word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghiịjklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "    \"\"\"Done the edit for up to 3, however this has substantially reduced the speed and time it takes\n",
    "    to generate a correction, so I will make it dormant for now\n",
    "    def edits3(word): \n",
    "        \"All edits that are three edits away from `word`.\"\n",
    "        return (e3 for e2 in edits2(word) for e3 in edits1(e2))\"\"\"\n",
    "\n",
    "    def edits2(word): \n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "    \n",
    "    my_candidates= candidates(err_word)\n",
    "    \n",
    "    for el in list(my_candidates):\n",
    "        if el in words:\n",
    "            top_words[el]= my_ngram[el]\n",
    "           \n",
    "    top_words= sorted(top_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words= top_words[:5]\n",
    "    top_words_and_probabilities = dict(top_words) # This gives the top words as well as the necessary probabilities\n",
    "    top_words= list(dict(top_words)) \n",
    "    \n",
    "    #return candidates(err_word)\n",
    "    return top_words, top_words_and_probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_ranking (sugg,ngram_result):\n",
    "    \n",
    "    dic_sugg= {}\n",
    "    val= 0\n",
    "    word= \"\"\n",
    "    \n",
    "    for el in sugg:   \n",
    "        dic_sugg[el]= ngram_result[el]\n",
    "    \n",
    "    for el in dic_sugg:\n",
    "        if dic_sugg[el]> val:\n",
    "            val= dic_sugg[el]\n",
    "            word = el\n",
    "    \n",
    "    dic = dict(sorted(dic_sugg.items()))# This is not functioning right\n",
    "         \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker no update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"To check for error word, for each word query if it is stored in the dictionary, else, \n",
    "get the error word and store its index\"\"\"\n",
    "\n",
    "import re\n",
    "\"\"\"\n",
    "import all you need here\n",
    "\"\"\"\n",
    "\n",
    "def spell_checker():\n",
    "    sentence=  input(\"Enter text here:\")\n",
    "    sentence= sentence.lower()\n",
    "    #sent=re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úùṅṇÄÁÀÉÈỌỌ̀ÒÓỊ̀ỊÌỤÚṄŃḾ’']\",'',sentence)#Use regex to clean sentence\n",
    "    sent=re.sub(r\"[^\\w\\s’]\",'',sentence)#Use regex to clean sentence\n",
    "    \n",
    "    \n",
    "\n",
    "    # I would have to turn My_error_sentence to a list\n",
    "    arr_for_sents = sent.split()\n",
    "    arr_for_sents_safe_keep= sent.split()\n",
    "\n",
    "    ### **** dictionary_words_for_edit= \"add file for corpus here for edit distance\"\n",
    "    #print(arr_for_sents)\n",
    "\n",
    "\n",
    "    # Here I initialize the index so that I can store the position of the number so as to change it when the correction is generated \n",
    "    # I also keep a list for possible candidates, so I can keep lists for certain indexes in it. I could make this a dictionary, with the index being the key\n",
    "    arr_index= 0\n",
    "    poss_candidates= {}\n",
    "    \n",
    "\n",
    "\n",
    "    for word in arr_for_sents:\n",
    "\n",
    "        if t.search(word) == False:\n",
    "\n",
    "\n",
    "            arr_index= arr_for_sents.index(word) #store the array index   \n",
    "            \n",
    "            ngram= ngram_multi_sugg(sent,arr_index) #get ngram suggestions\n",
    "            \n",
    "            candidates,candidates_and_prob= candidate_suggestions(ngram,word) #Get the suggestions\n",
    "            \n",
    "            correct_word= candidate_ranking(candidates,ngram) #Get the correct word\n",
    "            \n",
    "\n",
    "            #add suggestions to dictionary\n",
    "            poss_candidates[arr_index]= candidates#result[1]\n",
    "\n",
    "            #replace most-likely word with word in index \n",
    "            arr_for_sents[arr_index]= correct_word#result[0]\n",
    "\n",
    "            #return array index to zero \n",
    "            arr_index = 0 \n",
    "\n",
    "\n",
    "\n",
    "    #print(poss_candidates)\n",
    "    #print(arr_index)\n",
    "\n",
    "\n",
    "    #print(f\"Corrected Text: {' '.join(arr_for_sents_safe_keep)}\")# Prints error sentence\n",
    "    print(f\"Corrected Text: {' '.join(arr_for_sents)}\")# prints updated sentence\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(poss_candidates)\n",
    "\n",
    "\n",
    "    #print out error word and candidate suggestions\n",
    "    for el in poss_candidates:\n",
    "        print(f\"\\nError word:'{arr_for_sents_safe_keep[el]}'\\n->suggestions: {poss_candidates[el]}\\nCorrection: '{arr_for_sents[el]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker with Uploading sentence\n",
    "\n",
    "1. This version updates any error word it cannot find back into the sentence.\n",
    "2. It also updates the sentence as the correction is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To check for error word, for each word query if it is stored in the dictionary, else, \n",
    "get the error word and store its index\"\"\"\n",
    "\n",
    "import re\n",
    "from colorama import Fore, Back, Style\n",
    "#print(Fore.RED + 'some red text')\n",
    "\"\"\"\n",
    "import all you need here\n",
    "\"\"\"\n",
    "\n",
    "def spell_checker():\n",
    "    sentence=  input(\"Enter text here:\")\n",
    "    main_sentence= sentence\n",
    "    \n",
    "    sentence= sentence.lower()\n",
    "    #sent=re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úùṅṇÄÁÀÉÈỌỌ̀ÒÓỊ̀ỊÌỤÚṄŃḾ’']\",'',sentence)#Use regex to clean sentence\n",
    "    sent=re.sub(r\"[^\\w\\s’]\",'',sentence)#Use regex to clean sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # I would have to turn My_error_sentence to a list\n",
    "    arr_for_sents = sent.split()\n",
    "    arr_for_sents_safe_keep= sent.split()\n",
    "\n",
    "    ### **** dictionary_words_for_edit= \"add file for corpus here for edit distance\"\n",
    "    #print(arr_for_sents)\n",
    "\n",
    "\n",
    "    # Here I initialize the index so that I can store the position of the number so as to change it when the correction is generated \n",
    "    # I also keep a list for possible candidates, so I can keep lists for certain indexes in it. I could make this a dictionary, with the index being the key\n",
    "    arr_index= 0\n",
    "    poss_candidates= {}\n",
    "    \n",
    "\n",
    "\n",
    "    for word in arr_for_sents:\n",
    "\n",
    "        if t.search(word) == False:\n",
    "\n",
    "\n",
    "            arr_index= arr_for_sents.index(word) #store the array index   \n",
    "            \n",
    "            ngram= ngram_multi_sugg(sent,arr_index) #get ngram suggestions\n",
    "            \n",
    "            candidates,candidates_and_prob= candidate_suggestions(ngram,word) #Get the suggestions\n",
    "            \n",
    "            correct_word= candidate_ranking(candidates,ngram) #Get the correct word\n",
    "            \n",
    "\n",
    "            #add suggestions to dictionary\n",
    "            poss_candidates[arr_index]= candidates  #result[1]\n",
    "            \n",
    "            if candidates == []:\n",
    "                arr_for_sents[arr_index]= (Fore.RED + word+Style.RESET_ALL) #check how to return a colored word back\n",
    "                \n",
    "            else: \n",
    "                #replace most-likely word with word in index \n",
    "                arr_for_sents[arr_index]= (Fore.BLUE + correct_word +Style.RESET_ALL)#correct_word #result[0]\n",
    "\n",
    "            #return array index to zero \n",
    "            arr_index = 0 \n",
    "            \n",
    "        #update sent\n",
    "        sent=' '.join(arr_for_sents)\n",
    "\n",
    "\n",
    "\n",
    "    #print(poss_candidates)\n",
    "    #print(arr_index)\n",
    "\n",
    "\n",
    "    #print(f\"Corrected Text: {' '.join(arr_for_sents_safe_keep)}\")# Prints error sentence\n",
    "    \n",
    "    print(f\"\\nCorrected Text: {' '.join(arr_for_sents)}\")# prints updated sentence\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(poss_candidates)\n",
    "\n",
    "\n",
    "    #print out error word and candidate suggestions\n",
    "    for el in poss_candidates:\n",
    "        print(f\"\\n \\nError word:'{arr_for_sents_safe_keep[el]}'\\n->suggestions: {poss_candidates[el]}\\nCorrection: '{arr_for_sents[el]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text here:hk\n",
      "\n",
      "Corrected Text: \u001b[31mhk\u001b[0m\n",
      "\n",
      " \n",
      "Error word:'hk'\n",
      "->suggestions: []\n",
      "Correction: '\u001b[31mhk\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "spell_checker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Getting predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from colorama import Fore, Back, Style\n",
    "#print(Fore.RED + 'some red text')\n",
    "\"\"\"\n",
    "import all you need here\n",
    "\"\"\"\n",
    "\n",
    "def spell_checker_1(sentence):\n",
    "    main_sentence= sentence\n",
    "    \n",
    "    sentence= sentence.lower()\n",
    "    #sent=re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úùṅṇÄÁÀÉÈỌỌ̀ÒÓỊ̀ỊÌỤÚṄŃḾ’']\",'',sentence)#Use regex to clean sentence\n",
    "    sent=re.sub(r\"[^\\w\\s’]\",'',sentence)#Use regex to clean sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # I would have to turn My_error_sentence to a list\n",
    "    arr_for_sents = sent.split()\n",
    "    arr_for_sents_safe_keep= sent.split()\n",
    "\n",
    "    ### **** dictionary_words_for_edit= \"add file for corpus here for edit distance\"\n",
    "    #print(arr_for_sents)\n",
    "\n",
    "\n",
    "    # Here I initialize the index so that I can store the position of the number so as to change it when the correction is generated \n",
    "    # I also keep a list for possible candidates, so I can keep lists for certain indexes in it. I could make this a dictionary, with the index being the key\n",
    "    arr_index= 0\n",
    "    poss_candidates= {}\n",
    "    \n",
    "\n",
    "\n",
    "    for word in arr_for_sents:\n",
    "\n",
    "        if t.search(word) == False:\n",
    "\n",
    "\n",
    "            arr_index= arr_for_sents.index(word) #store the array index   \n",
    "            \n",
    "            ngram= ngram_multi_sugg(sent,arr_index) #get ngram suggestions\n",
    "            \n",
    "            candidates,candidates_and_prob= candidate_suggestions(ngram,word) #Get the suggestions\n",
    "            \n",
    "            correct_word= candidate_ranking(candidates,ngram) #Get the correct word\n",
    "            \n",
    "\n",
    "            #add suggestions to dictionary\n",
    "            poss_candidates[arr_index]= candidates  #result[1]\n",
    "            \n",
    "            if candidates == []:\n",
    "                arr_for_sents[arr_index]= (Fore.RED + word+Style.RESET_ALL) #check how to return a colored word back\n",
    "                \n",
    "            else: \n",
    "                #replace most-likely word with word in index \n",
    "                arr_for_sents[arr_index]= (Fore.BLUE + correct_word +Style.RESET_ALL)#correct_word #result[0]\n",
    "\n",
    "            #return array index to zero \n",
    "            arr_index = 0 \n",
    "            \n",
    "            try:\n",
    "                # block raising an exception\n",
    "                \"IndexError: list index out of range\"\n",
    "            except:\n",
    "                pass # doing nothing on exception\n",
    "            \n",
    "        #update sent\n",
    "        sent=' '.join(arr_for_sents)\n",
    "\n",
    "\n",
    "\n",
    "    #print(poss_candidates)\n",
    "    #print(arr_index)\n",
    "\n",
    "\n",
    "    #print(f\"Corrected Text: {' '.join(arr_for_sents_safe_keep)}\")# Prints error sentence\n",
    "    \n",
    "    \n",
    "    #print(f\"\\nCorrected Text: {' '.join(arr_for_sents)}\")# prints updated sentence\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(poss_candidates)\n",
    "    #error_words = ['ie', 'nị']\n",
    "    #suggestions = [['ihe', 'ike', 'ime', 'iwe', 'e'], ['ndị', 'na', 'dị', 'gị', 'sị']]\n",
    "\n",
    "    #output = list(zip(error_words, suggestions))\n",
    "\n",
    "   \n",
    "    \n",
    "    err=[]\n",
    "    sugg=[]\n",
    "    \n",
    "    for el in poss_candidates:\n",
    "        err.append(arr_for_sents_safe_keep[el])\n",
    "        sugg.append(poss_candidates[el])\n",
    "        \n",
    "    output= list(zip(err, sugg))\n",
    "\n",
    "    #print(output)\n",
    "    return output\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #print out error word and candidate suggestions\n",
    "    #for el in poss_candidates:\n",
    "       # print(f\"\\n \\nError word:'{arr_for_sents_safe_keep[el]}'\\n->suggestions: {poss_candidates[el]}\\nCorrection: '{arr_for_sents[el]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= open(\"text_file_modified.txt\",encoding=\"utf8\")\n",
    "#sent= open(\"test.ig.with.errors.txt\",encoding=\"utf8\")\n",
    "#sent= open(\"testing_my_test_set_err.txt\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "\n",
    "sent= sent.read()\n",
    "#sent= sent[:1000]\n",
    "\n",
    "def checker(x):\n",
    "\n",
    "    x= x.split(\"\\n\")\n",
    "    my_arr=[]\n",
    "    \n",
    "    for el in x: \n",
    "        x= spell_checker_1(el)\n",
    "        \n",
    "        \"\"\"This code was put here so that the function\n",
    "        will not include words with no suggestions\n",
    "        if len(x)>0:\n",
    "            combined_data = (el, x)\n",
    "            my_arr.append(combined_data)\n",
    "            \"\"\"\n",
    "        combined_data = (el, x)\n",
    "        my_arr.append(combined_data)\n",
    "        \n",
    "        \n",
    "    return my_arr\n",
    "  \n",
    "\n",
    "#Remove el with one el in the array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data=  checker(sent)\n",
    "#predicted_data= predicted_data [:5]\n",
    "#predicted_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngold_data=[('iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ ',\\n  'iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ'),\\n ('site naka onye nta akụkọ teta ', 'site naka onye nta akụkr teta'),\\n ('ọ dị paul mkpa ịgbanwe bọlb ọkụ eletrik dị nihu ụlọ ya ',\\n  'ọ dị paul mkpa ịgbanwe bọlb ọkụ eletrik dị nihu ụlọ ya'),\\n ('ọ dịkwa ya mkpa ihicha windo ndị dị nụlọ elu site nèzí nwunye ya ekwuwo nke a ọtụtụ ugboro ',\\n  'ọ dịkwa ya mkpa ihicha winda ndị dị nụọ elu site nèzí nwunye ya ekwuwo nke a ọtụtụ ugboro')]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = open(\"test.ig.txt\",encoding=\"utf8\")\n",
    "#file1 = open(\"testing_my_test_set_corr.txt\",encoding=\"utf8\")\n",
    "file1= file1.read()\n",
    "file1= file1.split(\"\\n\")\n",
    "file1_sample= file1[:10]\n",
    "file1_sample\n",
    "\n",
    "#file2 = open(\"test.ig.with.errors.txt\",encoding=\"utf8\")# This is supposed to be the sentences the ai corrects\n",
    "file2 = open(\"text_file_modified.txt\",encoding=\"utf8\")# This is supposed to be the sentences the ai corrects\n",
    "file2= file2.read()\n",
    "file2= file2.split(\"\\n\")\n",
    "file2_sample= file2[:10]\n",
    "file2_sample\n",
    "#tuple(file2_sample)\n",
    "\n",
    "#initial version without removing single-word sentences\n",
    "\"\"\"\n",
    "def gold_data(x,y):\n",
    "    \n",
    "    ex=[]\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        z= x[i]+\",\"+ y[i]\n",
    "        z=z.split(\",\")\n",
    "        z=tuple(z)\n",
    "        ex.append(z)\n",
    "        \n",
    "    return ex\n",
    "\"\"\"\n",
    "\n",
    "def gold_data(x,y):\n",
    "    \n",
    "    ex=[]\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if len(x[i])>1 and len(y[i])>1:\n",
    "            z= x[i]+\",\"+ y[i]\n",
    "            z=z.split(\",\")\n",
    "            z=tuple(z)\n",
    "            ex.append(z)\n",
    "        \n",
    "    return ex\n",
    "\n",
    "#gold_data = gold_data(file2,file1)\n",
    "#gold_data= gold_data[:5]\n",
    "gold_data= file1\n",
    "\"\"\"\n",
    "gold_data=[('iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ ',\n",
    "  'iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ'),\n",
    " ('site naka onye nta akụkọ teta ', 'site naka onye nta akụkr teta'),\n",
    " ('ọ dị paul mkpa ịgbanwe bọlb ọkụ eletrik dị nihu ụlọ ya ',\n",
    "  'ọ dị paul mkpa ịgbanwe bọlb ọkụ eletrik dị nihu ụlọ ya'),\n",
    " ('ọ dịkwa ya mkpa ihicha windo ndị dị nụlọ elu site nèzí nwunye ya ekwuwo nke a ọtụtụ ugboro ',\n",
    "  'ọ dịkwa ya mkpa ihicha winda ndị dị nụọ elu site nèzí nwunye ya ekwuwo nke a ọtụtụ ugboro')]\"\"\"\n",
    "\n",
    "#len(file1),len(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.9861553030635785\n",
      "precision: 0.9719577131873555\n",
      "accuracy: 0.9637921802900957\n",
      "fi_score: 0.9790050372561605\n",
      "tp= 79519\n",
      "tn= 561291\n",
      "fp= 7880\n",
      "fn= 16194\n"
     ]
    }
   ],
   "source": [
    "# ERROR CORRECTION Tp-Fn\n",
    "# tp: where the model correctly corrects an error_word will be a situation where the correct word is in the list of suggestions\n",
    "# fp: where a incorrect word is incorrected by the model (word in incorrect but correction not in sugg)\n",
    "# fn: where an correct word is corrected by the model (word in correct? but not in identified_error)\n",
    "# tn: where a correct word is ignored (Word in correct and not in identified_errors)\n",
    "\n",
    "\n",
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "tn=0\n",
    "\n",
    "\n",
    "#first I get the correct and incorrect words by default sentence\n",
    "# Then I get the predicted words\n",
    "# for the words in incorrect, if word in pred_err and suggestion is true elsefn, then tp (this means error is correct)\n",
    "# for words in incorrect, if word not in pred_errors(i.e incorrect word not flagged), then fn (this means error is correct and no sugg)\n",
    "# for worsd in correct, if word in pred_errors(a correct word is flagged)then fp, (this means there is no error, error is incorrect ie unavailable)\n",
    "# for words in correct, if word not in pred_errors(correct word not flagged) then tn\n",
    "\n",
    "gold_sentence=[]\n",
    "pred_sentence=[]\n",
    "pred_error_words=[]\n",
    "\n",
    "for (gold_sent), (pred_sent, pred_errors) in zip(gold_data, predicted_data):\n",
    "    gold_sentence.append(gold_sent)\n",
    "    pred_sentence.append(pred_sent)\n",
    "    pred_error_words.append(pred_errors)\n",
    "\n",
    "tp=0\n",
    "tn=0\n",
    "fn=0\n",
    "fp=0\n",
    "correct_words=[]\n",
    "incorrect_words=[]\n",
    "identified_error_words=[]\n",
    "suggestions=[]\n",
    "\n",
    "#print(pred_sentence)\n",
    "\n",
    "for el in range(0,len(gold_sentence)): \n",
    "    \n",
    "    #get the correct and incorrect words for each sentence\n",
    "    for word in pred_sentence[el].split():\n",
    "        #print(word)     \n",
    "        if word in gold_sentence[el]:\n",
    "            correct_words.append(word)\n",
    "            \n",
    "        else:\n",
    "            incorrect_words.append(word)\n",
    "    #print(correct_words)\n",
    "    #print(incorrect_words)\n",
    "    \n",
    "    for arr in pred_error_words[el]:\n",
    "        identified_error_words.append(arr[0])\n",
    "        suggestions.append(arr[1])\n",
    "    #print(identified_error_words) \n",
    "    #print(suggestions)  \n",
    "    \n",
    "    \n",
    "    # for the words in incorrect, if word in pred_err and suggestion is true elsefn, then tp (this means error is correct)\n",
    "    # for words in incorrect, if word not in pred_errors(i.e incorrect word not flagged), then fn (this means error is correct and no sugg)\n",
    "    # for worsd in correct, if word in pred_errors(a correct word is flagged)then fp, (this means there is no error, error is incorrect ie unavailable)\n",
    "    # for words in correct, if word not in pred_errors(correct word not flagged) then tn\n",
    "    \n",
    "    for word in incorrect_words:\n",
    "        \n",
    "        #print(word)\n",
    "        if word in identified_error_words:\n",
    "            #print(word)\n",
    "            x= gold_sentence[el].split()\n",
    "            indx= pred_sentence[el].split().index(word)\n",
    "            indx2= identified_error_words.index(word)\n",
    "            #print(suggestions[indx2])\n",
    "            \n",
    "            if x[indx] in suggestions[indx2]:\n",
    "                tp+=1\n",
    "                #if any(suggestion in gold_words for suggestion in suggestions):\n",
    "            else:\n",
    "                fn+=1           \n",
    "        else:\n",
    "            fn+=1\n",
    "            \n",
    "        \n",
    "    for word in correct_words:\n",
    "        \n",
    "        if word in identified_error_words:\n",
    "            fp+=1\n",
    "        else:\n",
    "            tn+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    correct_words=[]\n",
    "    incorrect_words=[]\n",
    "    identified_error_words=[]\n",
    "    suggestions=[]\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "#Initial result for 5-words suggestions\n",
    "recall: 0.8512874463564019\n",
    "precision: 0.6482874050100741\n",
    "accuracy: 0.9123453190073499\n",
    "fi_score: 0.7360471923267439\n",
    "tp= 81727\n",
    "tn= 528372\n",
    "fp= 44339\n",
    "fn= 14277\n",
    "\"\"\" \n",
    "    \n",
    "\n",
    "\n",
    "#recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "#precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "precision = tn / (tn + fn) if (tn + fn) != 0 else 0\n",
    "recall = tn / (tn + fp) if (tn + fp) != 0 else 0 \n",
    "accuracy= (tp+tn)/(tp + fp + fn + tn) if (tp + fp + fn + tn)!= 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "print(f'recall: {recall}\\nprecision: {precision}\\naccuracy: {accuracy}\\nfi_score: {f1_score}')\n",
    "print(f'tp= {tp}\\ntn= {tn}\\nfp= {fp}\\nfn= {fn}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr:569171\n",
      "incorr:95713\n",
      "iden:96250\n"
     ]
    }
   ],
   "source": [
    "#ERROR DETECTION\n",
    "\n",
    "gold_sentence=[]\n",
    "pred_sentence=[]\n",
    "pred_error_words=[]\n",
    "for (gold_sent), (pred_sent, pred_errors) in zip(gold_data, predicted_data):\n",
    "    gold_sentence.append(gold_sent)\n",
    "    pred_sentence.append(pred_sent)\n",
    "    pred_error_words.append(pred_errors)\n",
    "\n",
    "\n",
    "tp_d=0\n",
    "tn_d=0\n",
    "fn_d=0\n",
    "fp_d=0\n",
    "correct_words=[]\n",
    "incorrect_words=[]\n",
    "identified_error_words=[]\n",
    "corr= 0\n",
    "incorr= 0\n",
    "iden= 0\n",
    "\n",
    "\n",
    "\n",
    "for el in range(0,len(gold_sentence)): \n",
    "    \n",
    "    #get the correct and incorrect words for each sentence\n",
    "    for word in pred_sentence[el].split():\n",
    "        #print(word)     \n",
    "        if word in gold_sentence[el]:\n",
    "        #if t.search(word):\n",
    "            correct_words.append(word)\n",
    "            corr+=1\n",
    "        else:\n",
    "            incorrect_words.append(word)\n",
    "            incorr+=1\n",
    "    #print(correct_words)\n",
    "    #print(incorrect_words)\n",
    "    \n",
    "    for arr in pred_error_words[el]:\n",
    "        identified_error_words.append(arr[0])\n",
    "        iden+=1\n",
    "    #print(identified_error_words)\n",
    "    #All operations to include the tp-fn are to be here\n",
    "    \n",
    "    # ERROR DETECTION Tp-Fn\n",
    "\n",
    "    #tp and fn: Check if any of the correct words are among the error words. if valid word is err:add to fn else tp\n",
    "    for word in correct_words:\n",
    "        if word in identified_error_words:\n",
    "            #fn.append(word)\n",
    "            fn_d+=1\n",
    "            \n",
    "        else:\n",
    "            #tp.append(word)\n",
    "            tp_d+=1\n",
    "   \n",
    "\n",
    "    #tn and fp: Check if any of the error words are missing among the error words, add to fp, else tn\n",
    "    for word in incorrect_words:\n",
    "        if word in identified_error_words:\n",
    "            #tn.append(word)\n",
    "            tn_d+=1\n",
    "        else:\n",
    "            #fp.append(word)\n",
    "            fp_d+=1\n",
    "        \n",
    "        \n",
    "    correct_words=[] #This prevents the whole words from all senteces being uploaded together\n",
    "    incorrect_words=[]\n",
    "    identified_error_words=[]\n",
    "\n",
    "\n",
    " #ERROR CCORRECTION\n",
    "\n",
    "\"\"\"\n",
    "#Get the index of incorrect words\n",
    "incorrect_words=['quck', 'fx', 'jmps', 'ovr', 'dg']\n",
    "\n",
    "indx = 0  \n",
    "\n",
    "for word in pred_sentence[0].split():\n",
    "    #print(word)\n",
    "    if word in incorrect_words:\n",
    "        indx= pred_sentence[0].split().index(word)\n",
    "        print(indx)\n",
    "\"\"\"\n",
    "        \n",
    "\n",
    "       \n",
    "print(f\"corr:{corr}\\nincorr:{incorr}\\niden:{iden}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.9232810589993\n",
      "precision: 0.9181298701298701\n",
      "fi_score: 0.920698259560436\n",
      "accuracy:0.9771042768362601\n",
      "tp:561291\n",
      "tn:88370\n",
      "fn:7880\n",
      "fp:7343\n"
     ]
    }
   ],
   "source": [
    "# ERROR DETECTION EVALUATION\n",
    "\n",
    "recall = tn_d / (tn_d + fp_d) if (tn_d + fp_d) != 0 else 0 \n",
    "#recall= tp_d/(tp_d + fn_d) if (tp_d + fn_d) != 0 else 0\n",
    "#precision = tp_d / (tp_d + fp_d) if (tp_d + fp_d) != 0 else 0\n",
    "precision = tn_d / (tn_d + fn_d) if (tn_d + fn_d) != 0 else 0\n",
    "accuracy= (tp_d+tn_d)/(tp_d + fp_d + fn_d + tn_d) if (tp_d + fp_d + fn_d + tn_d)!= 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "print(f'recall: {recall}\\nprecision: {precision}\\nfi_score: {f1_score}\\naccuracy:{accuracy}')\n",
    "print(f\"tp:{tp_d}\\ntn:{tn_d}\\nfn:{fn_d}\\nfp:{fp_d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggestion Adequacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8464831168831168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_suggestions=0\n",
    "suggestion_score=0\n",
    "for (gold_sentence), (pred_sentence, pred_error_words) in zip(gold_data, predicted_data):\n",
    "     #ERROR CCORRECTION\n",
    "\n",
    "    gold_words = gold_sentence.split()\n",
    "    pred_words = pred_sentence.split()\n",
    "    indx= []\n",
    "    suggestion=[]\n",
    "    #total_suggestions=[]\n",
    "    \n",
    "    for error_word, suggestions in pred_error_words:\n",
    "        #print(error_word)\n",
    "        if error_word in pred_words:\n",
    "            indx.append(pred_words.index(error_word))\n",
    "            suggestion.append(suggestions)\n",
    "            #total_suggestions.append(suggestions)#change to +=1\n",
    "            total_suggestions+=1\n",
    "    #print(indx)\n",
    "    #print(suggestion)\n",
    "    \"\"\"\n",
    "    -\tCorrect suggestion is in the first 3 suggestions = 1 \n",
    "    -\tCorrect suggestion is in the last 7 suggestions = 0.5 \n",
    "    -\tCorrect suggestion is not in any of the 5 suggestions = -0.5 \n",
    "    -\tNo suggestions are returned= 0\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(indx)):\n",
    "        #print(indx[i])\n",
    "        if len(suggestion[i]) == 0:\n",
    "            suggestion_score += 0\n",
    "        \n",
    "        elif gold_words[indx[i]] in suggestion[i][:2]:\n",
    "            #print(gold_words[indx[i]])\n",
    "            suggestion_score+=1\n",
    "        \n",
    "        elif gold_words[indx[i]] in suggestion[i][2:]:\n",
    "            suggestion_score += 0.5\n",
    "            \n",
    "        elif gold_words[indx[i]] not in suggestion[i]:#remeber 0.5 gets subtracted and also no score is added hence its more like a -1.5\n",
    "            suggestion_score -= 0.5\n",
    "            \n",
    "        else:\n",
    "            #suggestion_score += 0\n",
    "            pass\n",
    "        \n",
    "        \"\"\"#This code is the code to be reciprocated when adding any new conditions\n",
    "        elif gold_words[indx[i]] in suggestion[i][:1]:\n",
    "            suggestion_score+=1\n",
    "        \"\"\" \n",
    "        \n",
    "            \n",
    "            \n",
    "suggestion_adequacy= suggestion_score/total_suggestions\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(suggestion_score)\n",
    "#print(total_suggestions)\n",
    "print(suggestion_adequacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERROR DETECTION\n",
    "\n",
    "gold_sentence=[]\n",
    "pred_sentence=[]\n",
    "pred_error_words=[]\n",
    "for (gold_sent), (pred_sent, pred_errors) in zip(gold_data, predicted_data):\n",
    "    gold_sentence.append(gold_sent)\n",
    "    pred_sentence.append(pred_sent)\n",
    "    pred_error_words.append(pred_errors)\n",
    "    \n",
    "\n",
    "correct_words=[]\n",
    "incorrect_words=[]\n",
    "identified_error_words=[]\n",
    "all_words=[]\n",
    "\n",
    "for el in range(0,len(gold_sentence)): \n",
    "    err_sent=pred_sentence[el].split()\n",
    "    corr_sent= gold_sentence[el].split()\n",
    "    \n",
    "    #get the correct and incorrect words for each sentence\n",
    "    for word in pred_sentence[el].split():\n",
    "    #for i in range(0,len(corr_sent)):\n",
    "        #print(err_sent[i])  \n",
    "        all_words.append(word)\n",
    "        \n",
    "        if word in gold_sentence[el]:\n",
    "        #if t.search(word):\n",
    "            correct_words.append(word)\n",
    "            \n",
    "        else:\n",
    "            incorrect_words.append(word)\n",
    "    #print(correct_words)\n",
    "    #print(incorrect_words)\n",
    "    \n",
    "    for arr in pred_error_words[el]:\n",
    "        identified_error_words.append(arr[0])\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print (f\"corr:{len(correct_words)}\\nincorr:{len(incorrect_words)}\\n{len(all_words)}\\nident:{len(identified_error_words)}\")\n",
    "#print(incorrect_words)\n",
    "#print(len(pred_sentence))\n",
    "#print(len(gold_sentence))\n",
    "#print(gold_sentence)\n",
    "#print(pred_sentence)\n",
    "#print(len(gold_data))\n",
    "#print(len(predicted_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=\"mgbe e kwusịrị okwu ndị bú nọgidenụ na anwapụta ihe unu onwe unu bụ nakwa kwe ka okwu chineke na eduzi nzọụkwụ gị kwa ụbọchị nnọkọ nke ụtụtụ g eji okwu bú isi nke isiokwu ya bụ soro chineke na eje ije noge ọgba aghara mechie iji dezie ihe ọ bụla na ekwekọghị ekwekọ netiti vulgate na asụsụ ndị mbụ e ji dee bible nebrija gbara cisneros ume sị mụnyeghachi ọkụ abụọ nke okpukpe anyị asụsụ hibru na nke grik m ga achọta akwụkwọ e ji amụ yoruba ọ ma na e nweghị ihe a ga eji tụnyere aka ọ na enyere jizọs na ume ọ na agba ya gịnị na gịnị mere ka ụjọ tụọ mozis nwaanyị ahụ kwuru sị anyị na arịọ ka a gbatara anyị ọsọ enyemaka ọ dịkwa anyị oké mkpa b olee otú ndị izrel ga esi abụrụ jehova ndị dị nsọ ma mgbe obere oge gafere ha kwụsịrị ịṅụrị ọṅụ nihi n ihe ka agha ha njọ bidoziri ime na britain ọ bu ezie na a na eleda ha anya nihi ụzọ ndụ ha a gbaghaara ndị dị otú ahụ obi ha ziri ezi mmehie ha nihi okwukwe ha nwere nebe ọkpara chineke nọ mmetụta setan na emekwa ka ihe na akawanye nanị njọ ndị ọzọ na ebu ihe na adịghị mma nobi ekpe ekpere ma ọ dị njikere is nụzọ ọ bụla ruru ya aka mee ka anyị kwụsị irubere chineke isi otu nime ha bụ mmadụ na onye na abụghị di ya ma ọ bụ nwunye ya inwe mmekọahụ nụdị ya ọ bịa ya mere mgbe m ghọsịrị dibịa bekee ahọọrọ m ịga nihu na nkà mmụta ọgwụ site nịmụ banyere ọrịa ịmụ banyere ihe ndị e ji mara ọrịa ihe ndị na akpata ya na mmetụta ndị ọ na enwe ọ bụghị ihe ijuanya na ahụ ọkụ ji kwok kit unu ejiriwo ọ na anara ihe ì na echeta a na anụkarị ka ndị mmadụ na ewu ụdị ihe ndị a mgbe ụkọ ego dị o nweghị ihe na ere obi ụtọ karịa ịhụ ka ndị ị na amụrụ baịbụl gbanwee otú ha si ebi ndụ nihi n ha amatala jehova ka ọ dị ugbu a vatican bụ òtù na ekiri ihe a na eme mgbe nile ma ọ bụ mba na esoghị ná mba ndị so nòtù un nnapụta ngosipụta nke obiọma sitere nịhụnanya onye na eji ọmịiko ege ntị na eme ka ahú ruo ndị ọzọ ala kwụsie ike ná mkpebi gị gịnị mechara mee ma ndị ndú okpukpe ndị juu enweghị iche ha ji ha kpọrọ ò pụtara na ihe ọ chọburu ime adịghị mma mbanụ olee otú nwunye bú onye kraịst kwesịrị isi na emeso di ya na ekweghị ekwe otu ihe bụ na nke a na enye ohere iji kpebie kpam kpam ihe iseokwu ndị dị oké mkpa ndị nnupụisi setan kpalitere nogige iden olee ihe ị mụtara nekpere otu onye livaị kpere mgbe ọ na enweghị ike ịga nụlọ nọ chineke ọ dịghị ma ọlị ụkpụrụ ndị b isi nke bible bụ ndị nwere kpọmkwem ihe jikọrọ ha na mmekọrịta anyị na jehova a gwara anyị ka anyị gakwuru jenny bú agadi nwaanyị isi chara awọ bú onye ndị obodo ahụ olee uru ọ bara isi ihe ndị mejupụtara ọgwụ ụfọdụ a na agbanye nahú na abụwanye mmiri ọgwụ ndị a na esighị nọbara nweta o nwere ike ịbụ na ọ na eme gị ihere nihi y oké ìgwè mmadụ ga anọ ya ịnabata ndị e si nọnụ kpọlite mbanụ\"\n",
    "word= word.split()\n",
    "len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Testing\n",
    "- The single quotes in some words have to go\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= 'olee otú ị pụrụ isi mee ka ahú ike gị ka mma site n’ihe oriri ka mma' # Testing the whole of this sentence gives a list index out of range error. This is becasue of the quotes and the space. It was the regex\n",
    "y= 'n’ezie ọchịchị ọma nke solomọn wetaara ndị ọ na achị udo mee ka ha nọrọ ná nchebe meekwa ka ha nwee obi ụtọ'\n",
    "x.strip()\n",
    "print(x)\n",
    "q= 'olee otú ị pụrụ isi mee ka ahú ike gị ka mma site n’ihx orirx za mna'\n",
    "\n",
    "# I also need to write a code that performes the whole process for one word at a time\n",
    "\n",
    "#olee otú ị pụrụ isi mee ka ahú ike gị ka mma site n’ihe oriri ka mma \n",
    "# Old test set data with words of 1 occurence\n",
    "x= \"Iguzo mwuli akọnauche nke ndị mmadụ site n'ịchọ mgbanwe abụghị ịda iwu\"# I need to prevent my spell checker from removing the ('), I also need to remove words or sentences with low no of occurence (akọnauche)'\n",
    "x2= \"ya mere, mgbe i chere na ị na-agba nkịtị bụ okwu ngosi ogogo, ekwenyeghị m\"\n",
    "x3= \"iji wezụga obi abụọ, ebea bụ ihe ole na ole n'ime ndị ọzọ nwoke ma ọbụ nwaanyị ọbụla chọrọ ịzụahịa n'ala abụghị ebe a mụrụ ya:\" # ịzụahịa, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.search(\"ruo\")\n",
    "\n",
    "# 1. I need to correct the trie, it accepts any single letter of the alphabet as a correct word, I need to find the correct \n",
    "# single alphabets in Igbo and correct for the. This may not be necessary as the spell checker need not disqualiify any letter \n",
    "# as incorrect. I could identify the ones non-native to igbo and remove them from the trie e.g c,q,x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Does the correction have to make sense in the language or just make sense in context. I.e we correct an error from a sentence and it doesnt give us the word we are expecting from the edit, rather it gives another different word from the word we intended to correct to, but this word is still correct in the context.\n",
    "2. Do I need to return the text back to the specified format, i.e with the special characters and numbers etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "1. Norvig performs much better when used within the application as many sugestions are returned\n",
    "2. In the original dataset, the special characters set themselves apart with spaces, this makes it easy to use regex in processing them. This is important as a feauture of any dataset that we will be using for the work to train and test the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
